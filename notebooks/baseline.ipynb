{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "197f652a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('d:/Projects/synthetic-images-detection/data/datasets')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "SEED = 42\n",
    "DATASETS_DIR = Path(os.getcwd()).parent / \"data\" / \"datasets\"\n",
    "SET_NAMES = [\"train\", \"val\", \"test\"]\n",
    "DATASETS_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88dc44d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "from tqdm.notebook import tqdm\n",
    "import math\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "\n",
    "class DatasetUtil:\n",
    "    def __init__(self, dataset_dir, set_names):\n",
    "        self.dataset_dir = dataset_dir\n",
    "        self.set_names = set_names\n",
    "        _datasets = {\n",
    "            name: {\n",
    "                \"df\": self.read_dataset(dataset_dir.parent, file_name=name + \".csv\")\n",
    "            } \n",
    "            for name in self.set_names\n",
    "        }\n",
    "        for d in _datasets:\n",
    "            _datasets[d][\"emb_matrix\"] = self.load_embedding_matrix(_datasets[d][\"df\"], workers=16)\n",
    "        self.datasets = _datasets\n",
    "    \n",
    "    def get_dataset(self, name: str) -> tuple[pd.DataFrame, np.ndarray]:\n",
    "        data = self.datasets[name]\n",
    "        return data[\"df\"], data[\"emb_matrix\"]\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_embedding_matrix(\n",
    "        df: pd.DataFrame,\n",
    "        dtype: np.dtype = np.float32,\n",
    "        dim: int = 768,\n",
    "        workers: int = 16\n",
    "    ) -> np.ndarray:\n",
    "        memmap_paths = df[\"emb_memmap\"]\n",
    "        df[\"emb_idx\"] = np.arange(len(memmap_paths), dtype=int)\n",
    "\n",
    "        def read_emb(path: str) -> np.ndarray:\n",
    "            return np.fromfile(path, dtype=dtype, count=dim)\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=workers) as executor:\n",
    "            embeddings = list(tqdm(\n",
    "                executor.map(read_emb, memmap_paths),\n",
    "                total=len(memmap_paths),\n",
    "                desc=\"Loading embeddings\"\n",
    "            ))\n",
    "\n",
    "        return np.stack(embeddings, axis=0)\n",
    "\n",
    "    @staticmethod\n",
    "    def read_dataset(dir_path: Path, file_name: str = \"processed_with_embeddings.csv\") -> pd.DataFrame:\n",
    "        df = pd.read_csv(dir_path / file_name)\n",
    "        df[\"fp\"] = df[\"fp\"].apply(lambda x: DATASETS_DIR / Path(x))\n",
    "        df[\"emb_memmap\"] = df[\"emb_memmap\"].apply(lambda x: DATASETS_DIR / Path(x))\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def save_dataset(df: pd.DataFrame, target_path: Path) -> None:\n",
    "        df[\"fp\"] = df[\"fp\"].apply(lambda x: Path(x).relative_to(DATASETS_DIR))\n",
    "        df[\"lr_fp\"] = df[\"lr_fp\"].apply(lambda x: Path(x).relative_to(DATASETS_DIR))\n",
    "        df[\"emb_memmap\"] = df[\"emb_memmap\"].apply(lambda x: Path(x).relative_to(DATASETS_DIR))\n",
    "        df.to_csv(target_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7aece0de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\grafit24\\AppData\\Local\\Temp\\ipykernel_19812\\3986819989.py:54: DtypeWarning: Columns (3,15,16,17,18) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(dir_path / file_name)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13c24680dc624f8e95879a10a5dedd2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading embeddings:   0%|          | 0/198272 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1a46612bda64bab95f403435d7e0aba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading embeddings:   0%|          | 0/18084 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71b8c91569fe480b957f8fecbf2b1ff2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading embeddings:   0%|          | 0/162773 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "datasets = DatasetUtil(DATASETS_DIR, SET_NAMES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88a1f8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9c397700",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\grafit24\\AppData\\Local\\Temp\\ipykernel_19812\\1417810168.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sota_test_df.emb_idx = range(len(sota_emb))\n"
     ]
    }
   ],
   "source": [
    "test_df, test_emb_matrix = datasets.get_dataset(\"test\")\n",
    "sota_test_df = test_df[test_df.model_name.isin([\"FLUX1_dev\", \"FLUX1_pro\"])]\n",
    "sota_emb = test_emb_matrix[sota_test_df.emb_idx]\n",
    "sota_test_df.emb_idx = range(len(sota_emb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5801454a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_xy(df, emb_matrix):\n",
    "    X = emb_matrix[df['emb_idx'].values]\n",
    "    y = df['label'].values\n",
    "    return X, y\n",
    "\n",
    "X_train, y_train = extract_xy(*datasets.get_dataset(\"train\"))\n",
    "X_val, y_val   = extract_xy(*datasets.get_dataset(\"val\"))\n",
    "X_test, y_test  = extract_xy(sota_test_df, sota_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "67301cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled   = scaler.transform(X_val)\n",
    "X_test_scaled  = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c5008e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed in 9.14 seconds\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(\n",
    "    solver='lbfgs',\n",
    "    max_iter=1000,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "clf.fit(X_train_scaled, y_train)\n",
    "print(f\"Training completed in {(time.time() - start_time):.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a3b04fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, X, y, split_name):\n",
    "    y_pred = model.predict(X)\n",
    "    y_prob = model.predict_proba(X)[:, 1]\n",
    "\n",
    "    print(f\"--- {split_name} Metrics ---\")\n",
    "    print(\n",
    "        f\"Accuracy  : {accuracy_score(y, y_pred):.4f}\\n\"\n",
    "        f\"Precision : {precision_score(y, y_pred):.4f}\\n\"\n",
    "        f\"Recall    : {recall_score(y, y_pred):.4f}\\n\"\n",
    "        f\"F1-score  : {f1_score(y, y_pred):.4f}\\n\"\n",
    "        f\"ROC AUC   : {roc_auc_score(y, y_prob):.4f}\\n\"\n",
    "    )\n",
    "    print(\"\\nClassification Report:\\n\", classification_report(y, y_pred, digits=4))\n",
    "    cm = confusion_matrix(y, y_pred)\n",
    "    print(\"Confusion Matrix:\\n\", cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5bfc1dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Validation Metrics ---\n",
      "Accuracy  : 0.9904\n",
      "Precision : 0.9868\n",
      "Recall    : 0.9941\n",
      "F1-score  : 0.9905\n",
      "ROC AUC   : 0.9996\n",
      "\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9941    0.9867    0.9904      9042\n",
      "           1     0.9868    0.9941    0.9905      9042\n",
      "\n",
      "    accuracy                         0.9904     18084\n",
      "   macro avg     0.9905    0.9904    0.9904     18084\n",
      "weighted avg     0.9905    0.9904    0.9904     18084\n",
      "\n",
      "Confusion Matrix:\n",
      " [[8922  120]\n",
      " [  53 8989]]\n",
      "--- Test Metrics ---\n",
      "Accuracy  : 0.9988\n",
      "Precision : 1.0000\n",
      "Recall    : 0.9988\n",
      "F1-score  : 0.9994\n",
      "ROC AUC   : nan\n",
      "\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.0000    0.0000    0.0000         0\n",
      "           1     1.0000    0.9988    0.9994      9777\n",
      "\n",
      "    accuracy                         0.9988      9777\n",
      "   macro avg     0.5000    0.4994    0.4997      9777\n",
      "weighted avg     1.0000    0.9988    0.9994      9777\n",
      "\n",
      "Confusion Matrix:\n",
      " [[   0    0]\n",
      " [  12 9765]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Projects\\synthetic-images-detection\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n",
      "d:\\Projects\\synthetic-images-detection\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "d:\\Projects\\synthetic-images-detection\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "d:\\Projects\\synthetic-images-detection\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Validation\n",
    "evaluate(clf, X_val_scaled, y_val, 'Validation')\n",
    "\n",
    "# Test\n",
    "evaluate(clf, X_test_scaled, y_test, 'Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321a9da8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
